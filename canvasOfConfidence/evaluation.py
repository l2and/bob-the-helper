import os
import json
import glob
from datetime import datetime
from typing import Dict, List, Any, Optional
from langsmith import Client, traceable
from langsmith.evaluation import evaluate
from langsmith.schemas import Dataset, Example
from langchain_anthropic import ChatAnthropic
import sys
import os
# Add happyLittleTreesOfKnowledge directory to path to import langgraph_agent
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'happyLittleTreesOfKnowledge'))
from langgraph_agent import get_agent  # Your enhanced agent
from evaluators import confidence_calibration, query_classification
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimplifiedBobRossEvaluator:
    """Enhanced evaluation with confidence-based metrics"""
    
    def __init__(self):
        self.client = Client(api_key=os.getenv("LANGSMITH_API_KEY"))
        self.agent = get_agent()  # Your enhanced agent
        self.llm = ChatAnthropic(
            anthropic_api_key=os.getenv("ANTHROPIC_API_KEY"),
            model_name="claude-sonnet-4-20250514"
        )
    
    def load_test_cases_from_csv(self, csv_file_path: str, dataset_name: str):
        """Load test cases from CSV file using LangSmith upload_csv method"""
        confidence_ranges = {
            "low": [0.0, 0.4],
            "medium": [0.4, 0.7], 
            "high": [0.7, 1.0]
        }
        
        try:
            # Check if dataset already exists
            try:
                existing_dataset = self.client.read_dataset(dataset_name=dataset_name)
                logger.info(f"Dataset {dataset_name} already exists. Skipping CSV upload to avoid duplicates.")
                return dataset_name
            except Exception:
                logger.info(f"Dataset {dataset_name} doesn't exist. Creating new dataset from CSV.")
            
            # Use LangSmith's upload_csv method
            dataset = self.client.upload_csv(
                csv_file=csv_file_path,
                input_keys=['input'],  # Column that contains the input text
                output_keys=['expected_query_type', 'expected_confidence', 'confidence_rationale', 
                           'category', 'severity', 'difficulty'],  # Expected output columns
                name=dataset_name,
                description="Support ticket evaluation dataset uploaded from CSV"
            )
            
            logger.info(f"Successfully uploaded CSV to dataset: {dataset_name}")
            logger.info(f"Dataset ID: {dataset.id}")
            
            return dataset_name
            
        except Exception as e:
            logger.error(f"Failed to upload CSV {csv_file_path}: {e}")
            raise

    
    
    
    
    @traceable(name="agent_factory")
    def agent_factory(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Enhanced factory function for evaluation with @traceable decorator"""
        try:
            # CSV upload uses column name as key, try both "text" and "input"
            text = inputs.get("text", "") or inputs.get("input", "")
            if not text:
                logger.warning(f"No text found in inputs. Available keys: {list(inputs.keys())}")
                return {
                    "query_type": "error",
                    "analysis": "No input text provided",
                    "overall_confidence": 0.0,
                    "classification_confidence": 0.0,
                    "context_confidence": 0.0
                }
            
            logger.info(f"Processing text: {text[:100]}...")  # Log first 100 chars for debugging
            
            # Use the process_highlighted_text method which is the correct interface
            result = self.agent.process_highlighted_text(text)
            
            # The result is already the final outputs dictionary
            final_outputs = result if isinstance(result, dict) else {}
            
            # Return in the expected format for evaluators
            return {
                "query_type": final_outputs.get("query_type", "unknown"),
                "overall_confidence": final_outputs.get("overall_confidence", 0.0),
                "analysis": final_outputs.get("bob_ross_response", ""),
                # Include other fields that evaluators might need
                "classification_confidence": final_outputs.get("classification_confidence", 0.0),
                "context_confidence": final_outputs.get("context_confidence", 0.0)
            }
        except Exception as e:
            logger.error(f"Error in enhanced agent factory: {e}")
            import traceback
            traceback.print_exc()
            return {
                "query_type": "error",
                "analysis": f"Error processing text: {e}",
                "error": str(e),
                "overall_confidence": 0.1,
                "classification_confidence": 0.0,
                "context_confidence": 0.0
            }
    
    def run_enhanced_evaluation(self, dataset_name: str) -> str:
        """Run the enhanced evaluation experiment"""
        try:
            logger.info("="*60)
            logger.info(f"üöÄ STARTING EVALUATION")
            logger.info(f"üìä Dataset: {dataset_name}")
            logger.info("="*60)
            
            # Get dataset info for logging
            try:
                dataset = self.client.read_dataset(dataset_name=dataset_name)
                example_count = len(list(self.client.list_examples(dataset_id=dataset.id)))
                logger.info(f"üìù Processing {example_count} test cases")
            except Exception as e:
                logger.warning(f"Could not count examples: {e}")
            
            # Simplified evaluator suite - only classification accuracy and confidence calibration
            evaluators = [
                query_classification,
                confidence_calibration
            ]
            logger.info(f"üîß Running 2 evaluators:")
            logger.info(f"   1Ô∏è‚É£ Query Classification Accuracy")
            logger.info(f"   2Ô∏è‚É£ Confidence Calibration")
            
            logger.info("‚è≥ Running evaluation... (this may take a few minutes)")
            
            # Use the evaluate function from langsmith.evaluation module
            from langsmith.evaluation import evaluate
            
            experiment_results = evaluate(
                self.agent_factory,
                data=dataset_name,
                evaluators=evaluators,
                experiment_prefix="bob-ross-simplified",
                description="Simplified evaluation with query classification and confidence calibration",
                max_concurrency=1,  # Process one at a time for debugging
                client=self.client
            )
            
            experiment_name = experiment_results.experiment_name
            logger.info(f"üî¨ Experiment results object: {type(experiment_results)}")
            logger.info(f"üî¨ Experiment results attributes: {dir(experiment_results)}")
            
            # Try to get results directly from experiment_results if available
            if hasattr(experiment_results, '_results'):
                logger.info(f"üî¨ Direct results available: {len(experiment_results._results) if experiment_results._results else 0}")
            if hasattr(experiment_results, '_summary_results'):
                logger.info(f"üî¨ Summary results available: {experiment_results._summary_results}")
            if hasattr(experiment_results, 'to_pandas'):
                logger.info("üî¨ Pandas conversion available")
            
            logger.info("="*60)
            logger.info(f"‚úÖ EVALUATION COMPLETE!")
            logger.info(f"üî¨ Experiment Name: {experiment_name}")
            logger.info("="*60)
            
            return experiment_name, experiment_results  # Return both for direct analysis
            
        except Exception as e:
            logger.error("="*60)
            logger.error(f"‚ùå EVALUATION FAILED: {e}")
            logger.error("="*60)
            raise
    
    def analyze_enhanced_results(self, experiment_name: str, experiment_results=None) -> Dict[str, Any]:
        """Analyze enhanced evaluation results with confidence insights"""
        try:
            logger.info("üîç ANALYZING RESULTS...")
            logger.info(f"üìà Fetching experiment data: {experiment_name}")
            
            # Try to use experiment_results directly first
            metrics = {
                "total_runs": 0,
                "query_classification": [],
                "confidence_calibration": [],
                "confidence_distribution": {"low": 0, "medium": 0, "high": 0}
            }
            
            if experiment_results and hasattr(experiment_results, 'to_pandas'):
                logger.info("üî¨ Using direct experiment results via pandas...")
                try:
                    # Wait for results to complete if needed
                    if hasattr(experiment_results, 'wait'):
                        logger.info("‚è≥ Waiting for experiment to complete...")
                        experiment_results.wait()
                    
                    # Convert to pandas DataFrame
                    df = experiment_results.to_pandas()
                    logger.info(f"üìä Found {len(df)} evaluation runs in DataFrame")
                    
                    if len(df) > 0:
                        logger.info(f"üîç DataFrame columns: {list(df.columns)}")
                        
                        # Extract metrics from DataFrame
                        metrics["total_runs"] = len(df)
                        
                        # Look for evaluator results in the DataFrame
                        for col in df.columns:
                            if 'query_classification' in col.lower():
                                # Boolean evaluator - convert True/False to 1.0/0.0 scores
                                bool_scores = df[col].dropna().tolist()
                                scores = [1.0 if score else 0.0 for score in bool_scores]
                                metrics["query_classification"].extend(scores)
                                logger.info(f"üîç Found query_classification results: {bool_scores} -> scores: {scores}")
                            elif 'confidence_calibration' in col.lower():
                                # Float evaluator - use scores directly
                                float_scores = df[col].dropna().tolist()
                                metrics["confidence_calibration"].extend(float_scores)
                                logger.info(f"üîç Found confidence_calibration scores: {float_scores}")
                        
                        # Analyze confidence distribution from outputs
                        if 'outputs.overall_confidence' in df.columns:
                            for confidence in df['outputs.overall_confidence'].dropna():
                                if confidence < 0.4:
                                    metrics["confidence_distribution"]["low"] += 1
                                elif confidence < 0.7:
                                    metrics["confidence_distribution"]["medium"] += 1
                                else:
                                    metrics["confidence_distribution"]["high"] += 1
                    
                except Exception as e:
                    logger.warning(f"Failed to use pandas approach: {e}")
                    logger.info("üîÑ Falling back to direct run query...")
            
            # Fallback: Try to query runs directly if pandas approach failed
            if metrics["total_runs"] == 0:
                logger.info("üîÑ Using fallback method to get runs...")
                import time
                time.sleep(3)  # Brief delay
                
                try:
                    runs = list(self.client.list_runs(project_name=experiment_name))
                    logger.info(f"üìä Found {len(runs)} evaluation runs via direct query")
                    
                    if len(runs) > 0:
                        metrics["total_runs"] = len(runs)
                        # Process runs for metrics (simplified version)
                        for run in runs:
                            if hasattr(run, 'feedback_stats') and run.feedback_stats:
                                feedback = run.feedback_stats
                                for metric_name in ["query_classification", "confidence_calibration"]:
                                    if metric_name in feedback:
                                        score = feedback[metric_name].get('avg', 0)
                                        metrics[metric_name].append(score)
                                        
                except Exception as e:
                    logger.warning(f"Fallback method also failed: {e}")
            
            # Calculate simplified results
            def safe_avg(scores): 
                return sum(scores) / len(scores) if scores else 0
            
            results = {
                "experiment_name": experiment_name,
                "total_test_cases": metrics["total_runs"],
                "query_classification": safe_avg(metrics["query_classification"]),
                "confidence_calibration": safe_avg(metrics["confidence_calibration"]),
                "confidence_distribution": metrics["confidence_distribution"],
                "overall_score": safe_avg([
                    safe_avg(metrics["query_classification"]),
                    safe_avg(metrics["confidence_calibration"])
                ])
            }
            
            logger.info("="*60)
            logger.info("üé® EVALUATION RESULTS:")
            logger.info("="*60)
            logger.info(f"üéØ Query Classification: {results['query_classification']:.3f} ({results['query_classification']*100:.1f}%)")
            logger.info(f"üìä Confidence Calibration: {results['confidence_calibration']:.3f} ({results['confidence_calibration']*100:.1f}%)")
            logger.info(f"üìà Overall Score: {results['overall_score']:.3f} ({results['overall_score']*100:.1f}%)")
            logger.info(f"üìã Test Cases Processed: {results['total_test_cases']}")
            logger.info("="*60)
            
            return results
            
        except Exception as e:
            logger.error(f"Error analyzing enhanced results: {e}")
            return {"error": str(e)}

def load_all_sample_datasets(evaluator, sample_dir="sample_datasets") -> str:
    """Load CSV files from sample_datasets directory into single dataset"""
    dataset_name = "bob-ross-support-tickets"
    
    # Find all CSV files in the sample_datasets directory
    csv_files = glob.glob(os.path.join(os.getcwd(), sample_dir, "*.csv"))
    
    if not csv_files:
        logger.warning(f"No CSV files found in {sample_dir} directory")
        return None
    
    logger.info(f"Found {len(csv_files)} CSV files to process")
    
    # Process each CSV file (though typically you'll have one main file)
    for csv_file in csv_files:
        try:
            logger.info(f"Processing {csv_file}...")
            evaluator.load_test_cases_from_csv(csv_file, dataset_name)
            
        except Exception as e:
            logger.error(f"Failed to load {csv_file}: {e}")
    
    return dataset_name

def main():
    """Simplified evaluation main function"""
    try:
        print("="*80)
        print("üé® BOB ROSS SUPPORT TICKETS EVALUATION")
        print("="*80)
        
        evaluator = SimplifiedBobRossEvaluator()
        
        # Load all sample datasets into single dataset
        logger.info("üìÇ STEP 1: Loading CSV datasets...")
        dataset_name = load_all_sample_datasets(evaluator)
        
        if not dataset_name:
            logger.error("‚ùå No datasets loaded. Please add CSV files to the sample_datasets directory.")
            return
        
        logger.info(f"‚úÖ Dataset loaded: {dataset_name}")
        
        # Run evaluation on the consolidated dataset
        logger.info("üîÑ STEP 2: Running evaluation...")
        
        try:
            # Run evaluation
            experiment_name, experiment_results = evaluator.run_enhanced_evaluation(dataset_name)
            
            # Analyze results
            logger.info("üìä STEP 3: Analyzing results...")
            results = evaluator.analyze_enhanced_results(experiment_name, experiment_results)
            
            # Save results in timestamped directory
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_dir = ".evaluation_results"
            os.makedirs(results_dir, exist_ok=True)
            
            results_filename = os.path.join(results_dir, f"{timestamp}.json")
            with open(results_filename, "w") as f:
                json.dump(results, f, indent=2)
            
            logger.info(f"üíæ Results saved to: {results_filename}")
            
            print("\n" + "="*80)
            print("üéâ EVALUATION COMPLETE!")
            print("="*80)
            print(f"üî¨ Experiment: {experiment_name}")
            print(f"üìä Dataset: {dataset_name}")
            print(f"üéØ Query Classification: {results.get('query_classification', 0)*100:.1f}%")
            print(f"üìä Confidence Calibration: {results.get('confidence_calibration', 0)*100:.1f}%")
            print(f"üìà Overall Score: {results.get('overall_score', 0)*100:.1f}%")
            print(f"üìÅ Results: {results_filename}")
            print(f"üåê LangSmith: https://smith.langchain.com")
            print("="*80)
            
        except Exception as e:
            logger.error(f"‚ùå Failed to evaluate dataset {dataset_name}: {e}")
        
    except Exception as e:
        logger.error(f"‚ùå Evaluation setup failed: {e}")
        raise

if __name__ == "__main__":
    main()